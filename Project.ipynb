{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgPjfmK9OW7pABN4URZOKj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anannya233/basic-study/blob/master/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-7lqM00HEnW",
        "outputId": "85a66692-d60e-4abe-bc30-268ced73aefd"
      },
      "source": [
        "import itertools\r\n",
        "import os\r\n",
        "\r\n",
        "import matplotlib.pylab as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "\r\n",
        "print(\"TF version:\", tf.__version__)\r\n",
        "print(\"Hub version:\", hub.__version__)\r\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version: 2.4.1\n",
            "Hub version: 0.11.0\n",
            "GPU is NOT AVAILABLE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkdCJ9bVHg2K",
        "outputId": "af8af1ad-2363-45a1-f90d-516e06be3f32"
      },
      "source": [
        "model_name = \"mobilenet_v3_small_100_224\" # @param ['bit_s-r50x1', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'inception_v3', 'inception_resnet_v2', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152']\r\n",
        "\r\n",
        "model_handle_map = {\r\n",
        "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\r\n",
        "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\r\n",
        "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\r\n",
        "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\r\n",
        "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\r\n",
        "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\r\n",
        "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\r\n",
        "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\r\n",
        "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\r\n",
        "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\r\n",
        "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\r\n",
        "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\r\n",
        "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\r\n",
        "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\r\n",
        "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\r\n",
        "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\r\n",
        "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\r\n",
        "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\r\n",
        "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\r\n",
        "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\r\n",
        "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\r\n",
        "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\r\n",
        "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\r\n",
        "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\r\n",
        "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\r\n",
        "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\r\n",
        "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\r\n",
        "}\r\n",
        "\r\n",
        "model_image_size_map = {\r\n",
        "  \"efficientnet_b0\": 224,\r\n",
        "  \"efficientnet_b1\": 240,\r\n",
        "  \"efficientnet_b2\": 260,\r\n",
        "  \"efficientnet_b3\": 300,\r\n",
        "  \"efficientnet_b4\": 380,\r\n",
        "  \"efficientnet_b5\": 456,\r\n",
        "  \"efficientnet_b6\": 528,\r\n",
        "  \"efficientnet_b7\": 600,\r\n",
        "  \"inception_v3\": 299,\r\n",
        "  \"inception_resnet_v2\": 299,\r\n",
        "  \"nasnet_large\": 331,\r\n",
        "  \"pnasnet_large\": 331,\r\n",
        "}\r\n",
        "\r\n",
        "model_handle = model_handle_map.get(model_name)\r\n",
        "pixels = model_image_size_map.get(model_name, 224)\r\n",
        "\r\n",
        "print(f\"Selected model: {model_name} : {model_handle}\")\r\n",
        "\r\n",
        "IMAGE_SIZE = (pixels, pixels)\r\n",
        "print(f\"Input size {IMAGE_SIZE}\")\r\n",
        "\r\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected model: mobilenet_v3_small_100_224 : https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\n",
            "Input size (224, 224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNJKcxX5HzfW"
      },
      "source": [
        "data_dir = tf.keras.utils.get_file(\r\n",
        "    'flower_photos',\r\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\r\n",
        "    untar=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9oQ-J7oH-mP",
        "outputId": "153cd93f-1d3e-4e26-97a0-99172070c1c8"
      },
      "source": [
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\r\n",
        "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\r\n",
        "                   interpolation=\"bilinear\")\r\n",
        "\r\n",
        "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\r\n",
        "    **datagen_kwargs)\r\n",
        "valid_generator = valid_datagen.flow_from_directory(\r\n",
        "    data_dir, subset=\"validation\", shuffle=False, **dataflow_kwargs)\r\n",
        "\r\n",
        "do_data_augmentation = False\r\n",
        "if do_data_augmentation:\r\n",
        "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\r\n",
        "      rotation_range=40,\r\n",
        "      horizontal_flip=True,\r\n",
        "      width_shift_range=0.2, height_shift_range=0.2,\r\n",
        "      shear_range=0.2, zoom_range=0.2,\r\n",
        "      **datagen_kwargs)\r\n",
        "else:\r\n",
        "  train_datagen = valid_datagen\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "    data_dir, subset=\"training\", shuffle=True, **dataflow_kwargs)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 731 images belonging to 5 classes.\n",
            "Found 2939 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgI6dQcKJCxm"
      },
      "source": [
        "do_fine_tuning = False"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GEnHZ6_Ij80",
        "outputId": "917b7330-b0a3-4004-fc3a-84c7074c1a0d"
      },
      "source": [
        "print(\"Building model with\", model_handle)\r\n",
        "model = tf.keras.Sequential([\r\n",
        "    # Explicitly define the input shape so the model can be properly\r\n",
        "    # loaded by the TFLiteConverter\r\n",
        "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\r\n",
        "    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\r\n",
        "    tf.keras.layers.Dropout(rate=0.2),\r\n",
        "    tf.keras.layers.Dense(train_generator.num_classes,\r\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\r\n",
        "])\r\n",
        "model.build((None,)+IMAGE_SIZE+(3,))\r\n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building model with https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 1024)              1529968   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 5125      \n",
            "=================================================================\n",
            "Total params: 1,535,093\n",
            "Trainable params: 5,125\n",
            "Non-trainable params: 1,529,968\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVw2DLo7I7Y9"
      },
      "source": [
        "model.compile(\r\n",
        "  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9), \r\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\r\n",
        "  metrics=['accuracy'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8CztUJzJNMG",
        "outputId": "1d56e966-0a93-4f58-8722-5cc5fcbc9cc5"
      },
      "source": [
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\r\n",
        "validation_steps = valid_generator.samples // valid_generator.batch_size\r\n",
        "hist = model.fit(\r\n",
        "    train_generator,\r\n",
        "    epochs=5, steps_per_epoch=steps_per_epoch,\r\n",
        "    validation_data=valid_generator,\r\n",
        "    validation_steps=validation_steps).history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "91/91 [==============================] - 55s 555ms/step - loss: 1.1905 - accuracy: 0.5821 - val_loss: 0.7279 - val_accuracy: 0.8281\n",
            "Epoch 2/5\n",
            "73/91 [=======================>......] - ETA: 7s - loss: 0.6802 - accuracy: 0.8800"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Gy62YGZJSH-"
      },
      "source": [
        "plt.figure()\r\n",
        "plt.ylabel(\"Loss (training and validation)\")\r\n",
        "plt.xlabel(\"Training Steps\")\r\n",
        "plt.ylim([0,2])\r\n",
        "plt.plot(hist[\"loss\"])\r\n",
        "plt.plot(hist[\"val_loss\"])\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\r\n",
        "plt.xlabel(\"Training Steps\")\r\n",
        "plt.ylim([0,1])\r\n",
        "plt.plot(hist[\"accuracy\"])\r\n",
        "plt.plot(hist[\"val_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_ECHtYTJST0"
      },
      "source": [
        "def get_class_string_from_index(index):\r\n",
        "   for class_string, class_index in valid_generator.class_indices.items():\r\n",
        "      if class_index == index:\r\n",
        "         return class_string\r\n",
        "\r\n",
        "x, y = next(valid_generator)\r\n",
        "image = x[0, :, :, :]\r\n",
        "true_index = np.argmax(y[0])\r\n",
        "plt.imshow(image)\r\n",
        "plt.axis('off')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Expand the validation image to (1, 224, 224, 3) before predicting the label\r\n",
        "prediction_scores = model.predict(np.expand_dims(image, axis=0))\r\n",
        "predicted_index = np.argmax(prediction_scores)\r\n",
        "print(\"True label: \" + get_class_string_from_index(true_index))\r\n",
        "print(\"Predicted label: \" + get_class_string_from_index(predicted_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcqNhUMhJSX_"
      },
      "source": [
        "saved_model_path = f\"/tmp/saved_flowers_model_{model_name}\"\r\n",
        "tf.saved_model.save(model, saved_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}